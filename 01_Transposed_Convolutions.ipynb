{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMg272ev/7e2JrgXzy9a20f"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><h1>Transposed Convolutions</h1></center>\n",
        "\n",
        "In feed-forward neural networks and standard convolutional layers, we typically reduce the spatial size of the input — going from a large image or signal to a smaller representation (e.g., class scores).\n",
        "\n",
        "However, in generative models (e.g., image generation), we often need to go in the opposite direction i.e. from a low-dimensional representation (latent vector) back to a high-dimensional output (full-sized image).\n",
        "\n",
        "*Some generative models expand input dimensions using gradient-based optimization, relying on the backward pass of convolution layers. But there's a more direct way to do this during the forward pass: by using transposed convolution layers.*\n",
        "\n",
        "A transposed convolution performs the reverse operation of a standard convolution, effectively expanding the input.\n",
        "\n",
        "A convolution can be seen as a series of inner products, a transposed convolution can\n",
        "be seen as a weighted sum of translated kernels.\n"
      ],
      "metadata": {
        "id": "vgWLVwHT23Ty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compare on a simple 1d example the results of a standard and a transposed\n",
        "convolution:\n",
        "Where $a$ is an input and $w$ is kernel or weight Tensor."
      ],
      "metadata": {
        "id": "shGG7MR9EvmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "hydNjtHc6ofS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[[1., 0., 1., 1., 0., 1., 0.]]])\n",
        "w = torch.tensor([[[1., 2., 3.]]])\n",
        "print(\"Shape of a: \",a.shape, \"Shape of w: \", w.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p70VSncPEb_G",
        "outputId": "45d2b552-fed4-4e7f-bd50-1140836a4db3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of a:  torch.Size([1, 1, 7]) Shape of w:  torch.Size([1, 1, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard 1D Colvolution\n",
        "Standard_conv_out = F.conv1d(a, w)\n",
        "print(\"Output: \", Standard_conv_out, \"Shape of standard conv output: \",Standard_conv_out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ4txSWlE744",
        "outputId": "03f2a18a-cf53-4efe-9fab-e050038ca7ec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:  tensor([[[4., 5., 3., 4., 2.]]]) Shape of standard conv output:  torch.Size([1, 1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The size of the resultant tensor after applying standard convolution is **less** than the input tensor."
      ],
      "metadata": {
        "id": "g-B5H7lBGwRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transposed convolution:\n",
        "transpose_conv_out = F.conv_transpose1d(a, w)\n",
        "print(\"Output: \", transpose_conv_out, \"Shape of transpose conv output: \",transpose_conv_out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qDPtraeFUt-",
        "outputId": "f25bfa9b-c7ee-4ae6-904f-d2805f655499"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:  tensor([[[1., 2., 4., 3., 5., 4., 2., 3., 0.]]]) Shape of transpose conv output:  torch.Size([1, 1, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The size of the resultant tensor after applying transpose convolution is **more** than the input tensor."
      ],
      "metadata": {
        "id": "FuV4JHyoHJP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a crucial step in understanding transposed convolution:\n",
        "\n",
        "- Transposed convolution layers are implemented to mimic the backward pass of a standard convolution but done as a forward pass.\n",
        "\n",
        "- This is why they're used in generative models to expand a compact representation into a larger output.\n",
        "\n"
      ],
      "metadata": {
        "id": "XFV5Mu-LHxqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The class `nn.ConvTranspose1d` embeds that operation into a nn.Module.\n",
        "\n",
        "We create A ConvTranspose1d module:\n",
        "\n",
        "- 1 input channel and 1 output channel\n",
        "\n",
        "- kernel size = 3"
      ],
      "metadata": {
        "id": "0l7qCO87LIOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = nn.ConvTranspose1d(1, 1, kernel_size=3)\n",
        "a = torch.tensor([[[1., 0., 1., 1., 0., 1., 0.]]])"
      ],
      "metadata": {
        "id": "ciz1WmdjF9_l"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then manually set the parameters:\n",
        "- we makes the operation purely linear, with no bias.\n",
        "\n",
        "- The kernel is [1,2,1]\n"
      ],
      "metadata": {
        "id": "Z5KjJsH7Lo7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.autograd.no_grad():\n",
        "    m.bias.zero_()\n",
        "    m.weight.copy_(torch.tensor([[[1., 2., 1.]]]))"
      ],
      "metadata": {
        "id": "m7519MB_LUgu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = m(a)\n",
        "print(\"Output: \", y, \"Shape of transpose conv output: \",y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdFIA6F6L3fl",
        "outputId": "e6a557a6-101e-475a-dbf2-d44e42769ff7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:  tensor([[[1., 2., 2., 3., 3., 2., 2., 1., 0.]]],\n",
            "       grad_fn=<ConvolutionBackward0>) Shape of transpose conv output:  torch.Size([1, 1, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like regular convolutions, transposed convolutions use:\n",
        "\n",
        "- Stride: Controls the spacing between output positions.\n",
        "\n",
        "- Padding: Controls cropping of the output.\n",
        "\n",
        "- Dilation: Expands the kernel by inserting gaps (makes it sparse).\n",
        "\n",
        "But There’s a Twist\n",
        "- In standard convolution, stride and padding are defined with respect to the input.\n",
        "\n",
        "- In transposed convolution, they are defined with respect to the output.\n",
        "\n",
        "It affects how large the output will be, and how it is cropped or expanded.\n",
        "\n",
        "- Stride in transposed convolution: It spreads out the influence of each input value across a wider region in the output.\n",
        "\n",
        "- Padding in transposed convolution: It is often used to crop the output, not to preserve input size as in standard convolution.\n",
        "\n",
        "- Dilation: Same behavior as in convolution, stretches the kernel by inserting zeros between its elements."
      ],
      "metadata": {
        "id": "Ko5SqeQtNAY-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PeAJ1TGyL6NM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}