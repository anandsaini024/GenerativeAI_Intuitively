{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOx9zW+4AZZPptRoitPawZ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anandsaini024/GenerativeAI_Intuitively/blob/master/02_Deep_Autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In many real-world problems like:\n",
        "\n",
        "- generating realistic images,\n",
        "\n",
        "- removing noise from signals (e.g. from photos or audio),\n",
        "\n",
        "- or compressing data (like turning a full-resolution photo into a smaller file),\n",
        "\n",
        "we are dealing with very high-dimensional data for example, a $28 \\times 28$ image has 784 pixel values, each a dimension.\n",
        "\n",
        "However, these data often lie on a low-dimensional manifold inside this huge space. That means there's a smaller number of \"essential\" or \"meaningful\" factors (degrees of freedom) that describe the data. Think:\n",
        "\n",
        "- for a face image: head shape, nose size, eye color, etc.\n",
        "\n",
        "- for a spoken word: pitch, tone, speaker’s accent, etc.\n",
        "\n",
        "> Imagine you want to store and later reconstruct hand-written digits (like MNIST). Instead of remembering all 784 pixel values for each digit, you learn to represent each digit using just 8 numbers (say, height, slant, roundness, etc.). That's a massive compression if done right and that's the job of autoencoders."
      ],
      "metadata": {
        "id": "REoN5er3uF0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have:\n",
        "\n",
        "- Original space: $ \\chi \\subset \\mathbb{R}^2$\n",
        "- Latent space: $ \\mathcal{F} \\subset \\mathbb{R}^1$\n",
        "\n",
        "The encoder function:\n",
        "\n",
        "$$ \\mathbb{f}: \\chi \\rightarrow \\mathcal{F}$$\n",
        "\n",
        "maps the 2D points onto 1D latent representations while preserving the structure (so that nearby points on the spiral map to nearby points in 1D).\n",
        "\n",
        "This mapping is injective on the manifold, it doesn't collapse two different points on the spiral into one.\n",
        "\n",
        "> Imagine you have a spiral drawn on a piece of paper like a coiled rope lying flat. This spiral lives in a 2D world (paper), but the points along the spiral really only vary in one meaningful direction: along the curve of the spiral. So the intrinsic dimension is 1.\n",
        "\n",
        "The function $\\mathbb{f}$ acts like \"flattening out\" the spiral — unfolding it into a straight line (1D). Now, every point on the spiral corresponds to a unique point on the line, preserving the data's structure."
      ],
      "metadata": {
        "id": "sK3apoxT2wbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate new data, one can sample points in the latent space $\\mathcal{F}$, then use a decoder function $\\mathbb{g}$ to map them back to the original space\n",
        "$\\chi$. The decoder $\\mathbb{g}$ approximates the inverse of the encoder\n",
        "$\\mathbb{f}$ on the data manifold."
      ],
      "metadata": {
        "id": "NIwMwkv05JIx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7vjIwnfAufbg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}